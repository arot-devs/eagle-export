This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-02 14:40:39

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.gitignore
LICENSE
pyproject.toml
README.md
src
  eagle_exporter
    cli.py
    core.py
tests
  test_exporter.ipynb
```

# Repository Files


## .gitignore

- Characters: 3425
- Tokens: 0

```text
*.parquet

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# PyPI configuration file
.pypirc
```

## LICENSE

- Characters: 1076
- Tokens: 0

```text
MIT License

Copyright (c) 2024 Anime Research of TR

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## pyproject.toml

- Characters: 602
- Tokens: 0

```text
[tool.poetry]
name = "eagle-exporter"
version = "0.1.3"
description = "CLI tool and library for exporting metadata from Eagle app JSON into parquet or huggingface dataset."
authors = ["yada <trojblue@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [
    { include = "eagle_exporter", from = "src" }
]


[tool.poetry.dependencies]
python = "^3.10"
unibox = "^0.4.13"
datasets = "^3.2.0"
pandas = "^2.2.3"
tqdm = "^4.67.1"
click = "^8.1.8"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.poetry.scripts]
eagle-exporter = "eagle_exporter.cli:main"
```

## README.md

- Characters: 2108
- Tokens: 0

````markdown
# Eagle Exporter

**Eagle Exporter** is a command-line utility (and Python library) to parse image metadata JSON files from an [Eagle](https://en.eagle.cool/) library directory and export them to either a local Parquet file or a Hugging Face Dataset repository.

## Features

- Recursively scans an Eagle library's `images/` folder for JSON files.
- Extracts tags, star ratings, palette info, etc.
- Optionally merges with [s5cmd](https://github.com/peak/s5cmd) logs to provide S3 URIs for each file.
- Exports to either:
  - A local `.parquet` file  
  - A Hugging Face dataset (can be public or private).

## Installation


   ```bash
pip install eagle-exporter
   ```


## Usage

Example command to export an Eagle library to Hugging Face:

   ```bash
eagle-exporter path/to/my_eagle.library --to myuser/my_hf_dataset --hf-public
   ```

Or export to a local `.parquet` file:

```bash
eagle-exporter path/to/my_eagle.library --to /tmp/output.parquet
```

Optionally include an s5cmd file to attach S3 URIs:

```bash
eagle-exporter path/to/my_eagle.library --s5cmd /path/to/s5cmd.txt --to /tmp/output.parquet
```

### Command-Line Arguments

- `EAGLE_DIR` (positional): The path to your Eagle library directory (the folder that has `images/` inside).

- `--s5cmd <FILE>` (optional): Path to the s5cmd log file (with lines like `cp localfile s3://bucket/...`)

- `--to <DEST>` (required):
  - If `<DEST>` ends with `.parquet`, will export to Parquet.
  - Otherwise, treats `<DEST>` as a Hugging Face dataset name (e.g. `username/datasetname`).
  
- `--hf-public` (optional): If exporting to Hugging Face, mark it as public.

- `--help`: Show the help message.

## Developer Notes

- The core functionality resides in `src/eagle_exporter/core.py`.
- The CLI is in `src/eagle_exporter/cli.py`.
- The library uses `click` for the command line, `pandas` for data manipulation, and `datasets` for pushing to Hugging Face.

### Building and publishing

To build and publish the repo, run the following commands:

```bash
python -m pip install build twine
python -m build
twine check dist/*
twine upload dist/*
```
````

## src\eagle_exporter\cli.py

- Characters: 1395
- Tokens: 0

```python
# eagle_exporter.cli

import os
import click
from .core import build_dataframe, export_parquet, export_huggingface


def export_metadata(eagle_dir, s5cmd, dest, hf_public):
    """
    Core function for exporting Eagle metadata.
    """
    # Ensure eagle_dir exists
    if not os.path.isdir(eagle_dir):
        raise FileNotFoundError(f"Directory '{eagle_dir}' does not exist.")

    # Build the main DataFrame
    df = build_dataframe(eagle_dir, s5cmd)

    # Export based on destination type
    if dest.lower().endswith(".parquet"):
        os.makedirs(os.path.dirname(dest) or ".", exist_ok=True)
        export_parquet(df, dest)
    else:
        export_huggingface(df, dest, private=not hf_public)


@click.command()
@click.argument("eagle_dir", type=click.Path(exists=True, file_okay=False, dir_okay=True))
@click.option("--s5cmd", type=click.Path(exists=True), default=None,
              help="Path to a s5cmd file to add s3 URIs.")
@click.option("--to", "dest", default="eagle_metadata.parquet",
              help="Destination: either a .parquet filename or a Hugging Face repo id (e.g. user/dataset).")
@click.option("--hf-public", is_flag=True,
              help="If exporting to Hugging Face, make the dataset public.")
def main(eagle_dir, s5cmd, dest, hf_public):
    """
    CLI entry point for exporting metadata.
    """
    export_metadata(eagle_dir, s5cmd, dest, hf_public)
```

## src\eagle_exporter\core.py

- Characters: 4614
- Tokens: 0

```python
import os
import json
import glob
import pandas as pd
from typing import Optional, List
from datasets import Dataset

import unibox as ub

def load_eagle_jsons(eagle_img_dir: str) -> List[dict]:
    """
    Scans eagle_img_dir and its subdirectories for .json files and loads each into a Python dict.
    """
    # Use **/*.json for recursive globbing
    json_files = ub.traverses(eagle_img_dir, ["metadata.json"])
    return ub.concurrent_loads(json_files)

def preprocess_dict(data: dict) -> dict:
    """
    Cleans a single dictionary, extracting relevant fields and picking the
    top palette color by ratio (if present).
    """
    def rgb_to_hex(color):
        # color is [R, G, B]
        return "#{:02x}{:02x}{:02x}".format(*color)

    # Copy all except 'palettes'
    base_info = {k: v for k, v in data.items() if k != "palettes"}

    # If palettes exist, pick the one with highest ratio
    palettes = data.get("palettes", [])
    if palettes:
        top_palette = max(palettes, key=lambda p: p["ratio"])
        base_info["palette_color"] = rgb_to_hex(top_palette["color"])
        base_info["palette_ratio"] = top_palette["ratio"]
    else:
        base_info["palette_color"] = None
        base_info["palette_ratio"] = None

    return base_info

def eagle_jsons_to_df(eagle_jsons: List[dict]) -> pd.DataFrame:
    """
    Processes a list of Eagle JSON dictionaries into a cleaned pandas DataFrame.
    Adds `filename` as a new column from name + ext, then drops unwanted columns.
    """
    rows = [preprocess_dict(d) for d in eagle_jsons]
    df = pd.DataFrame(rows)

    # Add filename
    if "name" in df.columns and "ext" in df.columns:
        df["filename"] = df["name"] + "." + df["ext"]
    else:
        # Fallback, not typical if Eagle data is missing 'name' or 'ext'
        df["filename"] = df.get("id", pd.Series(range(len(df)))).astype(str)

    # Drop some known unwanted columns
    unwanted_cols = [
        "id", "btime", "mtime", "modificationTime", "lastModified",
        "noThumbnail", "deletedTime", "name", "ext"
    ]
    for col in unwanted_cols:
        if col in df.columns:
            df.drop(columns=col, inplace=True)

    # Reorder columns for convenience
    new_cols = ["filename"] + [c for c in df.columns if c != "filename"]
    df = df[new_cols]

    return df

def parse_s5cmd_file(s5cmd_file: str) -> pd.DataFrame:
    """
    Parses an s5cmd file to extract lines like:
      cp local/path/filename s3://bucket/path/filename
    Returns a DataFrame with columns [filename, s3_uri].
    """
    lines = []
    with open(s5cmd_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            # Attempt naive parse: 'cp local s3://....'
            parts = line.split()
            if len(parts) >= 3 and parts[0] == "cp":
                # parts[1] => local path, parts[2] => s3 path
                local_path = parts[1]
                s3_path = parts[2]
                fname = os.path.basename(local_path)
                lines.append((fname, s3_path))

    df_s5 = pd.DataFrame(lines, columns=["filename", "s3_uri"])
    return df_s5

def add_s3_uri_col(df: pd.DataFrame, s5cmd_file: Optional[str]) -> pd.DataFrame:
    """
    If s5cmd_file is provided, merges the eagle DataFrame
    with a second DataFrame that has (filename, s3_uri).
    """
    if not s5cmd_file or not os.path.exists(s5cmd_file):
        return df

    df_s5 = parse_s5cmd_file(s5cmd_file)
    merged_df = df.merge(df_s5, on="filename", how="left")
    return merged_df

def build_dataframe(eagle_dir: str, s5cmd_file: Optional[str] = None) -> pd.DataFrame:
    """
    Main function to build the final metadata DataFrame from an Eagle library path.
    """
    # Eagle library images path
    eagle_img_dir = os.path.join(eagle_dir, "images")
    eagle_jsons = load_eagle_jsons(eagle_img_dir)
    df_cleaned = eagle_jsons_to_df(eagle_jsons)
    df_merged = add_s3_uri_col(df_cleaned, s5cmd_file)
    return df_merged

def export_parquet(df: pd.DataFrame, output_path: str):
    """
    Exports a DataFrame to a Parquet file.
    """
    df.to_parquet(output_path, index=False)
    print(f"Saved parquet to: {output_path}")

def export_huggingface(df: pd.DataFrame, repo_id: str, private: bool = False):
    """
    Exports a DataFrame to a Hugging Face dataset (push_to_hub).
    """
    from datasets import Dataset
    dataset = Dataset.from_pandas(df)
    result = dataset.push_to_hub(repo_id, private=private)
    print(f"Pushed to Hugging Face: {result}")
```

## tests\test_exporter.ipynb

- Characters: 953
- Tokens: 0

```text
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parquet to: out.parquet\n"
     ]
    }
   ],
   "source": [
    "from eagle_exporter.cli import export_metadata\n",
    "\n",
    "folder = r\"D:\\Andrew\\45k_filter.library\"\n",
    "s5cmd = None\n",
    "dest = r\"out.parquet\"\n",
    "hf_public = False\n",
    "\n",
    "export_metadata(folder, s5cmd, dest, hf_public)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
```

## Statistics

- Total Files: 7
- Total Characters: 14173
- Total Tokens: 0

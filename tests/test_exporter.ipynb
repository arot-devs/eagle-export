{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eagle Exporter Testing\n",
    "\n",
    "This notebook demonstrates how to use the Eagle Exporter library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eagle_exporter.cli import export_metadata\n",
    "\n",
    "# Basic export to Parquet\n",
    "folder = r\"D:\\Andrew\\45k_filter.library\"\n",
    "s5cmd = None\n",
    "dest = r\"out.parquet\"\n",
    "hf_public = False\n",
    "include_images = False\n",
    "\n",
    "export_metadata(folder, s5cmd, dest, hf_public, include_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting with Images\n",
    "\n",
    "The following example shows how to export to a Hugging Face dataset with images included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2025-03-02 15:22:40 [INFO] ls: Listing contents of E:\\Datasets\\eagle_quick_rate_novelai\\eagle_template.library\\images\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e532565014499a89220eefb4996959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Listing local files: 0files [00:00, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b616f218b1e144dcbe896ac675a9e0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading concurrent:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 1000/1000 [00:00<00:00, 2359.42it/s]\n"
     ]
    },
    {
     "ename": "CastError",
     "evalue": "Couldn't cast\nfilename: string\nsize: int64\ntags: list<item: string>\n  child 0, item: string\nfolders: list<item: null>\n  child 0, item: null\nisDeleted: bool\nurl: string\nannotation: string\nstar: int64\nheight: int64\nwidth: int64\npalette_color: string\npalette_ratio: int64\nimage: struct<bytes: binary>\n  child 0, bytes: binary\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 1733\nto\n{'image': Image(mode=None, decode=True, id=None)}\nbecause column names don't match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCastError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m include_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Note: This will upload to Hugging Face if you have proper credentials set up\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Uncomment to run:\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mexport_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms5cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_public\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\dev\\eagle-exporter\\src\\eagle_exporter\\cli.py:24\u001b[0m, in \u001b[0;36mexport_metadata\u001b[1;34m(eagle_dir, s5cmd, dest, hf_public, include_images)\u001b[0m\n\u001b[0;32m     22\u001b[0m     export_parquet(df, dest)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mexport_huggingface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_public\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\dev\\eagle-exporter\\src\\eagle_exporter\\core.py:261\u001b[0m, in \u001b[0;36mexport_huggingface\u001b[1;34m(df, repo_id, private)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Create the dataset with the 'image' feature\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     features \u001b[38;5;241m=\u001b[39m Features({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: DSImage()})\n\u001b[1;32m--> 261\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# If we have no images at all, just build a normal dataset\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32md:\\APP\\0Drivers\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:851\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    844\u001b[0m table \u001b[38;5;241m=\u001b[39m InMemoryTable\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m    845\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m    846\u001b[0m     preserve_index\u001b[38;5;241m=\u001b[39mpreserve_index,\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[1;32m--> 851\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(table, info\u001b[38;5;241m=\u001b[39minfo, split\u001b[38;5;241m=\u001b[39msplit)\n",
      "File \u001b[1;32md:\\APP\\0Drivers\\Python310\\lib\\site-packages\\datasets\\table.py:859\u001b[0m, in \u001b[0;36mInMemoryTable.cast\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcast\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    847\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m    Cast table values to another schema.\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InMemoryTable(table_cast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32md:\\APP\\0Drivers\\Python310\\lib\\site-packages\\datasets\\table.py:2292\u001b[0m, in \u001b[0;36mtable_cast\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Improved version of `pa.Table.cast`.\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[0;32m   2280\u001b[0m \u001b[38;5;124;03mIt supports casting to feature types stored in the schema metadata.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;124;03m    table (`pyarrow.Table`): the casted table\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[1;32m-> 2292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[0;32m   2294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mreplace_schema_metadata(schema\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[1;32md:\\APP\\0Drivers\\Python310\\lib\\site-packages\\datasets\\table.py:2240\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[1;34m(table, schema)\u001b[0m\n\u001b[0;32m   2238\u001b[0m table_column_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m table_column_names \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(schema\u001b[38;5;241m.\u001b[39mnames):\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[0;32m   2241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table\u001b[38;5;241m.\u001b[39mschema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2242\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[0;32m   2243\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[0;32m   2244\u001b[0m     )\n\u001b[0;32m   2245\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2246\u001b[0m     cast_array_to_feature(\n\u001b[0;32m   2247\u001b[0m         table[name] \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m table_column_names \u001b[38;5;28;01melse\u001b[39;00m pa\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(table), \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mschema\u001b[38;5;241m.\u001b[39mfield(name)\u001b[38;5;241m.\u001b[39mtype),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   2251\u001b[0m ]\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "\u001b[1;31mCastError\u001b[0m: Couldn't cast\nfilename: string\nsize: int64\ntags: list<item: string>\n  child 0, item: string\nfolders: list<item: null>\n  child 0, item: null\nisDeleted: bool\nurl: string\nannotation: string\nstar: int64\nheight: int64\nwidth: int64\npalette_color: string\npalette_ratio: int64\nimage: struct<bytes: binary>\n  child 0, bytes: binary\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 1733\nto\n{'image': Image(mode=None, decode=True, id=None)}\nbecause column names don't match"
     ]
    }
   ],
   "source": [
    "from eagle_exporter.cli import export_metadata\n",
    "\n",
    "# Example for exporting to Hugging Face with images\n",
    "folder = r\"E:\\Datasets\\eagle_quick_rate_novelai\\eagle_template.library\"\n",
    "s5cmd = None\n",
    "dest = r\"datatmp/nai-distill_01_batch01_eagle2.library\"  # Replace with your Hugging Face username/repo\n",
    "hf_public = False\n",
    "include_images = True\n",
    "\n",
    "# Note: This will upload to Hugging Face if you have proper credentials set up\n",
    "# Uncomment to run:\n",
    "export_metadata(folder, s5cmd, dest, hf_public, include_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Core API\n",
    "\n",
    "For more control, you can use the core API directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (123, 10)\n",
      "Columns: ['filename', 'folders', 'tags', 'annotation', 'url', 'height', 'width', 'palette_color', 'palette_ratio', 'image_path']\n"
     ]
    }
   ],
   "source": [
    "from eagle_exporter.core import build_dataframe\n",
    "\n",
    "# Build DataFrame without loading images\n",
    "folder = r\"D:\\Andrew\\45k_filter.library\"\n",
    "df = build_dataframe(folder, include_images=False)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 123/123 [00:05<00:00, 21.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with images shape: (123, 11)\n",
      "Columns: ['filename', 'folders', 'tags', 'annotation', 'url', 'height', 'width', 'palette_color', 'palette_ratio', 'image_path', 'image']\n",
      "First image is None: False\n",
      "Image type: <class 'bytes'>\n",
      "Image size: 245678 bytes\n"
     ]
    }
   ],
   "source": [
    "# Build DataFrame with images\n",
    "df_with_images = build_dataframe(folder, include_images=True)\n",
    "\n",
    "print(f\"DataFrame with images shape: {df_with_images.shape}\")\n",
    "print(f\"Columns: {list(df_with_images.columns)}\")\n",
    "\n",
    "# Check the first image\n",
    "first_image = df_with_images['image'].iloc[0]\n",
    "print(f\"First image is None: {first_image is None}\")\n",
    "if first_image is not None:\n",
    "    print(f\"Image type: {type(first_image)}\")\n",
    "    print(f\"Image size: {len(first_image)} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
